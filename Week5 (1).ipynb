{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHmVDf33l5Jd"
      },
      "source": [
        "***Over View***\n",
        "\n",
        "i) Data Augmentation\n",
        "\n",
        "ii) Machine Learning Modeling:\n",
        "\n",
        ">Feature selection\n",
        "\n",
        ">Missing values\n",
        "\n",
        ">Filtering or normalization\n",
        "\n",
        ">Classification Models\n",
        "\n",
        "> Cross Validation\n",
        "\n",
        ">Evalutaion\n",
        "\n",
        ">Confusion Matrix\n",
        "\n",
        ">Accuracy, precision, recall, f-measure, AUC, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL7U39EHmq7a"
      },
      "source": [
        "**1) Data Augmentation on Text Data**\n",
        "\n",
        "Data Augmentation is the process that enables us to increase the size of the training data without actually collecting the data. But why do we need more data? The answer is simple — the more data we have, the better the performance of the model.\n",
        "Image data augmentation steps such as flipping, cropping, rotation, blurring, zooming, etc. helped tremendously in computer vision. Also, it is relatively easy to create augmented images but the same is not the case with Text data due to the complexities inherent in the language. We will use two methods, TextAttack (synonym based approach) library and Googletrans (transaltion based approach).\n",
        "\n",
        "\n",
        "TextAttack is a Python framework that can be used for data augmentation in Text. The textattack.Augmenter class provides six methods for data augmentation, WordNetAugmenter, EmbeddingAugmenter, CharSwapAugmenter, EasyDataAugmenter, CheckListAugmenter, and CLAREAugmenter.\n",
        "\n",
        " Googletrans is built on top of Google Translate API. This uses Google Translate Ajax API for language detection and translation.Usage\n",
        "The key parameters to translate() method are:\n",
        ">src: source language. Optional parameter as googletrans will detect it.\n",
        "\n",
        ">dest: destination language. Mandatory parameter.\n",
        "\n",
        ">text: the text to be translated from source language to the destination language. Mandatory parameter.\n",
        "\n",
        "let install the libraries to start the today's ICP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8dUiOr3l1Hn",
        "outputId": "c27e919d-2243-44a5-cca6-29111fc505be"
      },
      "source": [
        "!pip install textattack\n",
        "!pip uninstall googletrans\n",
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textattack in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: bert-score>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.3.13)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.8.1)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.10/dist-packages (from textattack) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.16.1)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.10/dist-packages (from textattack) (2.8.1)\n",
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.10/dist-packages (from textattack) (0.2.3)\n",
            "Requirement already satisfied: lru-dict in /usr/local/lib/python3.10/dist-packages (from textattack) (1.3.0)\n",
            "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.1.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.13.1)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.44.2)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.10/dist-packages (from textattack) (3.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.5)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (from textattack) (1.1)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (from textattack) (0.5.13)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.5.0)\n",
            "Requirement already satisfied: pinyin>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (0.4.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n",
            "Requirement already satisfied: OpenHowNet in /usr/local/lib/python3.10/dist-packages (from textattack) (2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (24.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.4.0->textattack) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2024.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.19.1)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.35.29)\n",
            "Requirement already satisfied: conllu<5.0.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.14)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (6.2.3)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (5.2.0)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.4)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.5.10)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (3.1)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.5.2)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.4.0)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.7.1)\n",
            "Requirement already satisfied: semver<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (3.0.2)\n",
            "Requirement already satisfied: bioc<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (2.1)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language-tool-python->textattack) (24.1.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language-tool-python->textattack) (0.44.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.4.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words->textattack) (0.6.2)\n",
            "Requirement already satisfied: anytree in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (2.12.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (71.0.4)\n",
            "Requirement already satisfied: jsonlines>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from bioc<3.0.0,>=2.0.0->flair->textattack) (4.0.0)\n",
            "Requirement already satisfied: intervaltree in /usr/local/lib/python3.10/dist-packages (from bioc<3.0.0,>=2.0.0->flair->textattack) (3.1.0)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.29 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (1.35.29)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair->textattack) (0.10.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.12.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect>=1.0.9->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.5.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair->textattack) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair->textattack) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair->textattack) (0.34.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.6)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair->textattack) (2.4.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.4.0->flair->textattack) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair->textattack) (5.9.5)\n",
            "Found existing installation: googletrans 3.1.0a0\n",
            "Uninstalling googletrans-3.1.0a0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/translate\n",
            "    /usr/local/lib/python3.10/dist-packages/googletrans-3.1.0a0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/googletrans/*\n",
            "Proceed (Y/n)? "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbWH8e8spWZN"
      },
      "source": [
        "from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, EasyDataAugmenter, CharSwapAugmenter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol0_qwjopgXD"
      },
      "source": [
        "text = \"Leadership requires two things: a vision of the world that does not yet exist and the ability to communicate it.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRhXPFR4prcY"
      },
      "source": [
        "aug = WordNetAugmenter()\n",
        "aug.augment(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6b_UPtFprke"
      },
      "source": [
        "aug = EmbeddingAugmenter()\n",
        "aug.augment(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxE-LS-YprnV"
      },
      "source": [
        "aug = EasyDataAugmenter()\n",
        "aug.augment(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX602BwmprrX"
      },
      "source": [
        "aug = CharSwapAugmenter()\n",
        "aug.augment(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1-CNSWdqqT-"
      },
      "source": [
        "import googletrans\n",
        "from googletrans import Translator\n",
        "translator = Translator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyk3ZQ8wruBo"
      },
      "source": [
        "print(googletrans.LANGUAGES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONIQ6-KRqqaf"
      },
      "source": [
        "origin_text = \"The role of a leader is not to come up with all the great ideas. The role of a leader is to create an environment in which great ideas can happen\"\n",
        "print(origin_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D71Ms-KTqqgW"
      },
      "source": [
        "# translate from English to Italian\n",
        "text_trans = translator.translate(text=origin_text, dest='it').text\n",
        "print(text_trans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNs8FQFYtFQJ"
      },
      "source": [
        "# translate back to English from Italian\n",
        "translator.translate(text=text_trans, dest='en').text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nDIQG1atXCM"
      },
      "source": [
        "# translate from English to Urdu\n",
        "text_trans = translator.translate(text=origin_text, dest='ur').text\n",
        "print(text_trans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GtVia_0tbC3"
      },
      "source": [
        "# translate back to English from Urdu\n",
        "translator.translate(text=text_trans, dest='en').text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S595JEkvtjMd"
      },
      "source": [
        "**Lets Start the Machine Learning Part Now**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JlWPov9to2v"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRvyPUOst8E5"
      },
      "source": [
        "**Data Set**\n",
        "\n",
        "The Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details. It is a binary (2-class) classification problem. The Pima Indian Diabetes Dataset, originally from the National Institute of Diabetes and Digestive and Kidney Diseases, contains information of 768 women from a population near Phoenix, Arizona, USA. The outcome tested was Diabetes, 258 tested positive and 500 tested negative. Therefore, there is one target (dependent) variable and the 8 attributes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39W6IabBt67v"
      },
      "source": [
        "df_pima=pd.read_csv('https://raw.githubusercontent.com/npradaschnor/Pima-Indians-Diabetes-Dataset/master/diabetes.csv')\n",
        "df_pima.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rbVFz8Ju-99"
      },
      "source": [
        "So you can see 8 different features labeled into the outcomes of 1 and 0 where 1 stands for the observation has diabetes, and 0 denotes the observation does not have diabetes\n",
        "\n",
        "lets explore the data more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ost9n72u_vj"
      },
      "source": [
        "df_pima.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWKmno6wvJNY"
      },
      "source": [
        "Check Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blvL01B-vFvw"
      },
      "source": [
        "df_pima.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMR45tIXvMkI"
      },
      "source": [
        "No missing values? Oh wait, but I just saw somebody's Blood Pressure as 0! LOL.\n",
        "\n",
        "The dataset is known to have missing values. Specifically, there are missing observations for some columns that are marked as a zero value. You can deduce this by the definition of those columns, and it is impractical to have a zero value is invalid for those measures, e.g., zero for body mass index or blood pressure is invalid.\n",
        "\n",
        "so lets fix this issue. Have to fill these 0's with NaN. Not changing Pregnancies and Outcome variable because 0 is a valid answer for both of them\n",
        "\n",
        "**Replacing 0's with 'NaN'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9ff4FCJv2ZM"
      },
      "source": [
        "df_pima['Glucose'] = df_pima['Glucose'].replace(0, np.nan)\n",
        "df_pima['BloodPressure'] = df_pima['BloodPressure'].replace(0, np.nan)\n",
        "df_pima['SkinThickness'] = df_pima['SkinThickness'].replace(0, np.nan)\n",
        "df_pima['Insulin'] = df_pima['Insulin'].replace(0, np.nan)\n",
        "df_pima['BMI'] = df_pima['BMI'].replace(0, np.nan)\n",
        "df_pima['DiabetesPedigreeFunction'] = df_pima['DiabetesPedigreeFunction'].replace(0, np.nan)\n",
        "df_pima['Age'] = df_pima['Age'].replace(0, np.nan)\n",
        "\n",
        "df_pima.head(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlyeD8cPw6IR"
      },
      "source": [
        "df_pima.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHPAamlZxstT"
      },
      "source": [
        "There you go. We have to now deal with these missing values in order to proceed.\n",
        "\n",
        "**Filling Missing Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDP4eOeaxpPB"
      },
      "source": [
        "df_pima['BMI'].fillna(df_pima['BMI'].median(), inplace=True)\n",
        "df_pima['Glucose'].fillna(df_pima['Glucose'].median(), inplace=True)\n",
        "df_pima['BloodPressure'].fillna(df_pima['BloodPressure'].median(), inplace=True)\n",
        "df_pima['SkinThickness'].fillna(df_pima['SkinThickness'].median(), inplace=True)\n",
        "df_pima['Insulin'].fillna(df_pima['Insulin'].median(), inplace=True)\n",
        "\n",
        "df_pima.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L0blSKwyNz3"
      },
      "source": [
        "Missing values are replaced by medians as seen above\n",
        "\n",
        "**Next lets make a corelation plot for feature selection**\n",
        "\n",
        "The importance of feature selection can best be recognized when you are dealing with a dataset that contains a vast number of features. This type of dataset is often referred to as a high dimensional dataset. Now, with this high dimensionality, comes a lot of problems such as - this high dimensionality will significantly increase the training time of your machine learning model, it can make your model very complicated which in turn may lead to Overfitting.\n",
        "Often in a high dimensional feature set, there remain several features which are redundant meaning these features are nothing but extensions of the other essential features. These redundant features do not effectively contribute to the model training as well. So, clearly, there is a need to extract the most important and the most relevant features for a dataset in order to get the most effective predictive modeling performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Remf-v1vyLvU"
      },
      "source": [
        "corr = df_pima[df_pima.columns].corr()\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "sns.heatmap(corr, cmap=cmap, annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g67oERrjy8Cr"
      },
      "source": [
        "**Feature selcetion with SelectKBest**\n",
        "\n",
        "Let's convert the DataFrame object to a NumPy array to achieve faster computation. Also, let's segregate the data into separate variables so that the features and the labels are separated, this will help in feature selection.The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features, in this case, it is Chi-Squared. First,we will implement a Chi-Squared statistical test for non-negative features to select 4 of the best features from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h4ZKVl3zuvf"
      },
      "source": [
        "array = df_pima.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm9dzmUve_G7"
      },
      "source": [
        "Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN4WZZ0YyfWr"
      },
      "source": [
        "# Import the necessary libraries first\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Feature extraction\n",
        "test = SelectKBest(score_func=chi2, k=5)\n",
        "fit = test.fit(X, Y)\n",
        "\n",
        "# Summarize scores\n",
        "np.set_printoptions(precision=3)\n",
        "print(fit.scores_)\n",
        "\n",
        "\n",
        "features = fit.transform(X)\n",
        "# Summarize selected features\n",
        "print(features[0:5,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK9HMx6sz4y9"
      },
      "source": [
        "Interpretation:\n",
        "You can see the scores for each attribute and the 4 attributes chosen (those with the highest scores): Pregnancies, Glucose, Insulin, and age. This scores will help you further in determining the best features for training your model. This scores will help you further in determining the best features for training your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2EVRsn6z6q1"
      },
      "source": [
        "df_pima"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epGSMp-h1vc_"
      },
      "source": [
        "Based on this analysis lets reduce our features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAkb8c9N1v2h"
      },
      "source": [
        "X_features = pd.DataFrame(data = df_pima, columns = [\"Pregnancies\",\"Glucose\",\"Insulin\",\"BMI\",\"Age\"])\n",
        "X_graph=X_features.copy()\n",
        "X_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHPn-P2m2Lfa"
      },
      "source": [
        "Y = df_pima.iloc[:,8]\n",
        "Y.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6klpn6O2lKI"
      },
      "source": [
        "**Data Normalization/Scaling**\n",
        "\n",
        "What is Feature Scaling?\n",
        "\n",
        "It refers to putting the values in the same range or same scale so that no variable is dominated by the other.\n",
        "\n",
        "Why Scaling?\n",
        "\n",
        "Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Euclidean distance between two data points in their computations, this is a problem.If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. To suppress this effect, we need to bring all features to the same level of magnitudes. This can be achieved by scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JA6lC8b23d0"
      },
      "source": [
        "#Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "X_features = scaler.fit_transform(X_features)\n",
        "X_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHr1pKNk3IPv"
      },
      "source": [
        "Split Data: Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3qKTpVu3JRn"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_features, Y, test_size=0.20, random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtOu4E-63TJ2"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "models = []\n",
        "\n",
        "models.append((\"Logistic Regression:\",LogisticRegression()))\n",
        "models.append((\"Support Vector Machine-linear:\",SVC(kernel=\"linear\",C=0.2)))\n",
        "models.append((\"Support Vector Machine-rbf:\",SVC(kernel=\"rbf\")))\n",
        "models.append((\"Random Forest:\",RandomForestClassifier(n_estimators=5)))\n",
        "models.append((\"eXtreme Gradient Boost:\",XGBClassifier()))\n",
        "\n",
        "print('Models appended...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIO1UIIQ3ng0"
      },
      "source": [
        "results = []\n",
        "names = []\n",
        "for name,model in models:\n",
        "    kfold = KFold(n_splits=5, random_state=3, shuffle = True)\n",
        "    cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
        "    names.append(name)\n",
        "    results.append(cv_result)\n",
        "for i in range(len(names)):\n",
        "    print(names[i],results[i].mean()*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTUfetBB8aXf"
      },
      "source": [
        "from sklearn import metrics\n",
        "from IPython.display import Image\n",
        "from sklearn import tree\n",
        "from pydotplus import graph_from_dot_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4JkmoVB6U6Q"
      },
      "source": [
        "# Instantiate Decision Tree with a max depth of 3\n",
        "tree_model = DecisionTreeClassifier(max_depth=3)\n",
        "# Fit a decision tree\n",
        "tree_model = tree_model.fit(X_train, Y_train)\n",
        "# Training accuracy\n",
        "tree_model.score(X_train, Y_train)\n",
        "\n",
        "# Predictions/probs on the test dataset\n",
        "predicted = pd.DataFrame(tree_model.predict(X_test))\n",
        "probs = pd.DataFrame(tree_model.predict_proba(X_test))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7j7u_wTk5Oz"
      },
      "source": [
        "probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zef2eTnW7p4R"
      },
      "source": [
        "# Score metrics\n",
        "tree_accuracy = metrics.accuracy_score(Y_test, predicted)\n",
        "tree_roc_auc = metrics.roc_auc_score(Y_test, probs[1])\n",
        "tree_confus_matrix = metrics.confusion_matrix(Y_test, predicted)\n",
        "tree_classification_report = metrics.classification_report(Y_test, predicted)\n",
        "tree_precision = metrics.precision_score(Y_test, predicted, pos_label=1)\n",
        "tree_recall = metrics.recall_score(Y_test, predicted, pos_label=1)\n",
        "tree_f1 = metrics.f1_score(Y_test, predicted, pos_label=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU_PqZzg858I"
      },
      "source": [
        "print(\"accuracy: \", tree_accuracy)\n",
        "print(\"AUC: \", tree_roc_auc)\n",
        "print(\"Precision: \",tree_precision)\n",
        "print(\"Recall: \", tree_recall)\n",
        "print(\"F1 Score: \", tree_f1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro24xqtR_DyS"
      },
      "source": [
        "print(tree_classification_report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU60tclP_R4h"
      },
      "source": [
        "print(tree_confus_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BcKWtXV_HuH"
      },
      "source": [
        "# Creating the confusion matrix graphs\n",
        "plt.figure(figsize=(10,8))\n",
        "ax=sns.heatmap(tree_confus_matrix,cmap=\"YlGnBu\", annot=True, fmt='d')\n",
        "ax.set(ylabel=\"True Label\", xlabel=\"Predicted Label\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYQS7ZXG7vt_"
      },
      "source": [
        "# output decision plot\n",
        "dot_data = tree.export_graphviz(tree_model, out_file=None,\n",
        "                     feature_names=X_graph.columns.tolist(),\n",
        "                     class_names=['Dibetic', 'NonDibetic'],\n",
        "                     filled=True, rounded=True,\n",
        "                     special_characters=True)\n",
        "graph = graph_from_dot_data(dot_data)\n",
        "graph.write_png(\"decision_tree.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Fine-tuned Random Forest\n",
        "rf_model_tuned = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)\n",
        "rf_model_tuned.fit(X_train, Y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "predicted_rf_tuned = rf_model_tuned.predict(X_test)\n",
        "probs_rf_tuned = rf_model_tuned.predict_proba(X_test)\n",
        "\n",
        "# Performance metrics\n",
        "rf_accuracy_tuned = metrics.accuracy_score(Y_test, predicted_rf_tuned)\n",
        "rf_roc_auc_tuned = metrics.roc_auc_score(Y_test, probs_rf_tuned[:, 1])\n",
        "rf_precision_tuned = metrics.precision_score(Y_test, predicted_rf_tuned)\n",
        "rf_recall_tuned = metrics.recall_score(Y_test, predicted_rf_tuned)\n",
        "rf_f1_tuned = metrics.f1_score(Y_test, predicted_rf_tuned)\n",
        "\n",
        "\n",
        "\n",
        "# Random Forest results\n",
        "print(f\"Accuracy: {rf_accuracy_tuned}\")\n",
        "print(f\"AUC: {rf_roc_auc_tuned}\")\n",
        "print(f\"Precision: {rf_precision_tuned}\")\n",
        "print(f\"Recall: {rf_recall_tuned}\")\n",
        "print(f\"F1 Score: {rf_f1_tuned}\")"
      ],
      "metadata": {
        "id": "rsl48gYu_K2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXHaiUah_Q5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}